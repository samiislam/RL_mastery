{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0 = no wind, 1 = 1 up, 2 = 2 up\n",
    "windy_gw = np.zeros((7, 10), dtype=np.int8)\n",
    "\n",
    "windy_gw[0:7, 3:6] = 1\n",
    "windy_gw[0:7, 8] = 1\n",
    "\n",
    "windy_gw[0:7, 6:8] = 2\n",
    "\n",
    "start_location = np.array([3, 0])\n",
    "\n",
    "goal = np.array([3, 7])\n",
    "\n",
    "actions = np.array([\n",
    "    [1, 0],     #up     - 0\n",
    "    [0, 1],     #right  - 1\n",
    "    [-1, 0],    #down   - 2\n",
    "    [0, -1],    #left   - 3\n",
    "\n",
    "    [1, 1],     # up-right\n",
    "    [-1, 1],    # right-down\n",
    "    [-1, -1],   # down-left\n",
    "    [1, -1]     # up-left\n",
    "])\n",
    "\n",
    "num_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_action_indices(grid_location):\n",
    "    valid = []\n",
    "    for i, action in enumerate(actions):\n",
    "        projected_location = grid_location + action\n",
    "        if projected_location[0] >= 0 and projected_location[0] < 7 and projected_location[1] >= 0 and projected_location[1] < 10:\n",
    "            valid.append(i)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469575eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 7x10 grid positions and 4 actions\n",
    "\n",
    "# π an arbitrary ε-soft policy\n",
    "policy = np.zeros((7, 10, num_actions), dtype=np.float32)\n",
    "\n",
    "for y in range(7):\n",
    "    for x in range(10):\n",
    "        valid = get_valid_action_indices(np.array([y, x]))\n",
    "        prob = 1.0 / len(valid)\n",
    "        for a in valid:\n",
    "            policy[y, x, a] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q(s, a) ∈ R (arbitrarily), for all s ∈ S, a ∈ A(s)\n",
    "action_value = np.zeros((7, 10, num_actions), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5 # step-size\n",
    "gamma = 1 # undiscounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5055aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_index(s, Q, epsilon):\n",
    "    Q_s = Q[tuple(s)]\n",
    "    valid_actions = np.array(get_valid_action_indices(s), dtype=int)\n",
    "    \n",
    "    if epsilon==0.0:\n",
    "        return valid_actions[np.argmax(Q_s[valid_actions])]\n",
    "    \n",
    "    k = len(valid_actions)\n",
    "\n",
    "    q_valid = Q_s[valid_actions]\n",
    "    max_q = np.max(q_valid)\n",
    "    greedy_actions = valid_actions[q_valid == max_q]   # possibly multiple\n",
    "    m = len(greedy_actions)\n",
    "\n",
    "    p_a = np.zeros(num_actions, dtype=np.float32)\n",
    "\n",
    "    # exploration mass over valid actions\n",
    "    p_a[valid_actions] = epsilon / k\n",
    "\n",
    "    # greedy mass split over all greedy ties\n",
    "    p_a[greedy_actions] += (1.0 - epsilon) / m\n",
    "\n",
    "    return np.random.choice(num_actions, p=p_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a378034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_destination_state(current_state, action_index):\n",
    "    dest_state = current_state + actions[action_index]\n",
    "    wind_strength = windy_gw[0, current_state[1]]  # wind by current column\n",
    "    return np.clip(dest_state + np.array([wind_strength, 0]), [0,0], [6, 9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_train(epsilon):\n",
    "    \"\"\"\n",
    "    Runs ONE SARSA episode\n",
    "\n",
    "    Returns:\n",
    "        episode: list of (state, action_idx, reward, next_state)\n",
    "    \"\"\"\n",
    "    state = start_location.copy()\n",
    "    action_index_per_Q = choose_action_index(state, action_value, epsilon)\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    while True:\n",
    "        next_state = evaluate_destination_state(state, action_index_per_Q)\n",
    "        done = np.array_equal(next_state, goal)\n",
    "        reward = 0 if done else -1\n",
    "\n",
    "        # record step\n",
    "        episode.append((state.copy(),\n",
    "                        int(action_index_per_Q),\n",
    "                        int(reward),\n",
    "                        next_state.copy()))\n",
    "\n",
    "        t_state_actionindex = tuple(np.concatenate((state, [action_index_per_Q]), axis=0))\n",
    "\n",
    "        if done:\n",
    "            # Q(S, A) <- Q(S, A) + α [R + γ*0 - Q(S, A)]\n",
    "            action_value[t_state_actionindex] += alpha * (reward - action_value[t_state_actionindex])\n",
    "            break\n",
    "            \n",
    "        next_action_index_per_Q = choose_action_index(next_state, action_value, epsilon)\n",
    "        t_next_state_actionindex = tuple(np.concatenate((next_state, [next_action_index_per_Q]), axis=0))\n",
    "\n",
    "        # Q(S, A) <- Q(S, A) + α [R + γQ(S´, A´) - Q(S, A)]\n",
    "        action_value[t_state_actionindex] += alpha * (\n",
    "            reward + (gamma * action_value[t_next_state_actionindex]) - action_value[t_state_actionindex]\n",
    "        )\n",
    "        \n",
    "\n",
    "        state, action_index_per_Q = next_state, next_action_index_per_Q\n",
    "\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fca242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_infer(max_steps=10_000):\n",
    "    state = start_location.copy()\n",
    "    episode = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = choose_action_index(state, action_value, epsilon=0.0)  # greedy\n",
    "        next_state = evaluate_destination_state(state, action)\n",
    "        reward = 0 if np.array_equal(next_state, goal) else -1\n",
    "\n",
    "        episode.append((state.copy(), int(action), int(reward), next_state.copy()))\n",
    "\n",
    "        state = next_state\n",
    "        if np.array_equal(state, goal):\n",
    "            break\n",
    "\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "def visualize_windy_episode(\n",
    "    episode,\n",
    "    wind_strengths,              # list/np.array length = cols (e.g., 10)\n",
    "    rows=7,\n",
    "    cols=10,\n",
    "    start=(3, 0),\n",
    "    goal=(3, 7),\n",
    "    action_deltas=None,          # dict or array mapping action_idx -> (dy, dx)\n",
    "    sleep=0.05,\n",
    "    show_wind=True,\n",
    "    show_trail=True,\n",
    "    title_prefix=\"Episode\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Animate one Windy Gridworld episode.\n",
    "    episode: list of tuples, each tuple at least (state, action_idx, reward, next_state/projected)\n",
    "    wind_strengths: per-column upward push (typically positive means push UP, i.e., dy increases if origin='lower')\n",
    "    start/goal: (y, x)\n",
    "    action_deltas: optional mapping to draw an arrow for the chosen action.\n",
    "                  Example for 4 actions: {0:(1,0), 1:(-1,0), 2:(0,-1), 3:(0,1)}\n",
    "                  Adjust to your encoding and y-axis convention.\n",
    "    \"\"\"\n",
    "\n",
    "    wind_strengths = np.asarray(wind_strengths, dtype=int)\n",
    "    assert wind_strengths.shape[0] == cols, \"wind_strengths must have length == cols\"\n",
    "\n",
    "    # Background grid codes: 0 empty, 1 start, 2 goal\n",
    "    grid = np.zeros((rows, cols), dtype=int)\n",
    "    grid[start] = 1\n",
    "    grid[goal] = 2\n",
    "\n",
    "    cmap = plt.cm.colors.ListedColormap([\"white\", \"#90ee90\", \"#f08080\"])  # empty, start, goal\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    fig.subplots_adjust(top=0.82)  # more headroom for the title\n",
    "\n",
    "    # Keep trail of visited cells\n",
    "    trail = []\n",
    "\n",
    "    for t, step in enumerate(episode):\n",
    "        # Unpack flexibly\n",
    "        # Expected: (state, action_idx, reward, next_state_or_projected)\n",
    "        state = step[0]\n",
    "        action_idx = step[1] if len(step) > 1 else None\n",
    "        reward = step[2] if len(step) > 2 else None\n",
    "        next_state = step[3] if len(step) > 3 else None\n",
    "\n",
    "        y, x = int(state[0]), int(state[1])\n",
    "        trail.append((y, x))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.imshow(grid, origin=\"lower\", cmap=cmap, vmin=0, vmax=2)\n",
    "\n",
    "        # Grid lines\n",
    "        ax.set_xticks(np.arange(-0.5, cols, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-0.5, rows, 1), minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"gray\", linewidth=0.5)\n",
    "        ax.tick_params(which=\"both\", length=0)\n",
    "        ax.set_xticks(np.arange(cols))\n",
    "        ax.set_yticks(np.arange(rows))\n",
    "\n",
    "        # Wind labels\n",
    "        if show_wind:\n",
    "            for cx in range(cols):\n",
    "                ax.text(cx, rows - 0.2, f\"{wind_strengths[cx]}\", ha=\"center\", va=\"top\", fontsize=10)\n",
    "            ax.text(-0.9, rows - 0.2, \"wind\", ha=\"left\", va=\"top\", fontsize=10)\n",
    "\n",
    "        # Trail (path so far)\n",
    "        if show_trail and len(trail) > 1:\n",
    "            xs = [p[1] for p in trail]\n",
    "            ys = [p[0] for p in trail]\n",
    "            ax.plot(xs, ys, linewidth=2)  # default color\n",
    "\n",
    "        # Current agent position\n",
    "        ax.plot(x, y, \"bo\", markersize=9)\n",
    "\n",
    "        # Next position marker (if present)\n",
    "        if next_state is not None:\n",
    "            ny, nx = int(next_state[0]), int(next_state[1])\n",
    "            ax.plot(nx, ny, \"rx\", markersize=9)\n",
    "\n",
    "        # Action arrow (optional)\n",
    "        if action_deltas is not None and action_idx is not None:\n",
    "            if isinstance(action_deltas, dict):\n",
    "                dy, dx = action_deltas[int(action_idx)]\n",
    "            else:\n",
    "                dy, dx = action_deltas[int(action_idx)]\n",
    "            # Arrow in grid coordinates\n",
    "            arrow = FancyArrowPatch(\n",
    "                (x, y), (x + dx * 0.6, y + dy * 0.6),\n",
    "                arrowstyle=\"->\", mutation_scale=12, linewidth=2\n",
    "            )\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "        # Title\n",
    "        info = f\"{title_prefix} | step {t}\"\n",
    "        if action_idx is not None:\n",
    "            info += f\" | a={action_idx}\"\n",
    "        if reward is not None:\n",
    "            info += f\" | r={reward}\"\n",
    "            \n",
    "        ax.set_title(info, pad=22)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        time.sleep(sleep)\n",
    "\n",
    "        # Stop early if terminal reached (optional safety)\n",
    "        if (y, x) == goal:\n",
    "            break\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "wind_strengths = windy_gw.max(axis=0)\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    episode = run_episode_train(epsilon=0.1)\n",
    "\n",
    "    # Optional visualization (e.g., every 10 episodes)\n",
    "    if e % 10 == 0:\n",
    "        visualize_windy_episode(\n",
    "            episode=episode,\n",
    "            wind_strengths=wind_strengths,\n",
    "            rows=7, cols=10,\n",
    "            start=tuple(start_location),\n",
    "            goal=tuple(goal),\n",
    "            action_deltas=actions,\n",
    "            sleep=0.03,\n",
    "            title_prefix=f\"Training ep {e}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aec752",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_infer = run_episode_infer()\n",
    "\n",
    "visualize_windy_episode(\n",
    "    episode=ep_infer,\n",
    "    wind_strengths=windy_gw.max(axis=0),\n",
    "    rows=7, cols=10,\n",
    "    start=tuple(start_location),\n",
    "    goal=tuple(goal),\n",
    "    action_deltas=actions,\n",
    "    sleep=0.08,\n",
    "    title_prefix=\"Inference\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_mastery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
