{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dae9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbed625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute all Poisson probabilities once (huge speedup!)\n",
    "num_req_ret = 11\n",
    "req1_probs = np.array([poisson.pmf(k, 3) for k in range(num_req_ret)])\n",
    "ret1_probs = np.array([poisson.pmf(k, 3) for k in range(num_req_ret)])\n",
    "req2_probs = np.array([poisson.pmf(k, 4) for k in range(num_req_ret)])\n",
    "ret2_probs = np.array([poisson.pmf(k, 2) for k in range(num_req_ret)])\n",
    "\n",
    "# Store V as a 2D array instead of list of objects\n",
    "V = np.zeros((21, 21), dtype=np.float64)\n",
    "policy = np.zeros((21, 21), dtype=np.int32)\n",
    "\n",
    "# Store policy history for visualization\n",
    "policy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value_vectorized(loc1, loc2, action, gamma=0.9):\n",
    "    \"\"\"Vectorized computation for a single state.\"\"\"\n",
    "    \n",
    "    net_move = action\n",
    "    loc1_today = loc1 - net_move\n",
    "    loc2_today = loc2 + net_move\n",
    "    \n",
    "    # Check validity\n",
    "    if loc1_today < 0 or loc2_today < 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Create all combinations using meshgrid (vectorized!)\n",
    "    rq1_vals = np.arange(num_req_ret)\n",
    "    re1_vals = np.arange(num_req_ret)\n",
    "    rq2_vals = np.arange(num_req_ret)\n",
    "    re2_vals = np.arange(num_req_ret)\n",
    "    \n",
    "    rq1, re1, rq2, re2 = np.meshgrid(rq1_vals, re1_vals, rq2_vals, re2_vals, indexing='ij')\n",
    "    \n",
    "    # Compute all probabilities at once (vectorized!)\n",
    "    probs = (req1_probs[rq1] * ret1_probs[re1] * \n",
    "             req2_probs[rq2] * ret2_probs[re2])\n",
    "    \n",
    "    # Compute actual rentals (vectorized!)\n",
    "    actual_rentals_1 = np.minimum(rq1, loc1_today)\n",
    "    actual_rentals_2 = np.minimum(rq2, loc2_today)\n",
    "    \n",
    "    # Compute rewards (vectorized!)\n",
    "    rewards = -2 * abs(net_move) + 10 * actual_rentals_1 + 10 * actual_rentals_2\n",
    "    \n",
    "    # Compute next states (vectorized!)\n",
    "    new_loc1 = np.clip(loc1_today - actual_rentals_1 + re1, 0, 20)\n",
    "    new_loc2 = np.clip(loc2_today - actual_rentals_2 + re2, 0, 20)\n",
    "    \n",
    "    # Get next state values (vectorized indexing!)\n",
    "    next_values = V[new_loc1, new_loc2]\n",
    "    \n",
    "    # Compute expected value (single sum over all combinations)\n",
    "    state_value = np.sum(probs * (rewards + gamma * next_values))\n",
    "    \n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dab5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_evaluation_vectorized(theta=0.01, gamma=0.9, max_iterations=100):\n",
    "    \"\"\"Policy evaluation with vectorized state value computation.\"\"\"\n",
    "    \n",
    "    global V\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = np.zeros_like(V)\n",
    "        \n",
    "        # Loop over states (can't easily vectorize this outer loop without huge memory)\n",
    "        for loc1 in range(21):\n",
    "            for loc2 in range(21):\n",
    "                action = policy[loc1, loc2]\n",
    "                V_new[loc1, loc2] = compute_state_value_vectorized(loc1, loc2, action, gamma)\n",
    "        \n",
    "        # Compute delta\n",
    "        delta = np.max(np.abs(V - V_new))\n",
    "        \n",
    "        #print(f\"Iteration {iteration + 1}: delta = {delta:.6f}\")\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        if delta < theta:\n",
    "            #print(f\"Converged after {iteration + 1} iterations!\")\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_values_vectorized(loc1, loc2, gamma=0.9):\n",
    "    \"\"\"Compute Q(s,a) for all actions at once.\"\"\"\n",
    "    \n",
    "    actions = np.array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\n",
    "    action_values = np.full(11, -np.inf)\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        # Check validity\n",
    "        loc1_after = loc1 - action\n",
    "        loc2_after = loc2 + action\n",
    "        \n",
    "        if loc1_after < 0 or loc2_after < 0:\n",
    "            continue\n",
    "        \n",
    "        action_values[i] = compute_state_value_vectorized(loc1, loc2, action, gamma)\n",
    "    \n",
    "    return actions[np.argmax(action_values)]\n",
    "\n",
    "def run_policy_improvement_vectorized(gamma=0.9):\n",
    "    \"\"\"Policy improvement with vectorized action selection.\"\"\"\n",
    "    \n",
    "    global policy\n",
    "    \n",
    "    policy_new = np.zeros_like(policy)\n",
    "    \n",
    "    for loc1 in range(21):\n",
    "        for loc2 in range(21):\n",
    "            policy_new[loc1, loc2] = compute_action_values_vectorized(loc1, loc2, gamma)\n",
    "    \n",
    "    # Check stability\n",
    "    policy_changes = np.sum(policy_new != policy)\n",
    "    print(f\"Policy changes: {policy_changes}\")\n",
    "    \n",
    "    policy = policy_new\n",
    "    \n",
    "    return policy_changes == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79542337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_iteration_vectorized(theta=0.01, gamma=0.9, max_iterations=100):\n",
    "    \"\"\"Complete policy iteration with vectorization.\"\"\"\n",
    "    \n",
    "    global policy_history\n",
    "    policy_history = []\n",
    "\n",
    "    # Save initial policy\n",
    "    policy_history.append(policy.copy())\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n=== Policy Iteration {iteration + 1} ===\")\n",
    "        \n",
    "        # Policy Evaluation\n",
    "        print(\"Running policy evaluation...\")\n",
    "        run_policy_evaluation_vectorized(theta, gamma)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        print(\"Running policy improvement...\")\n",
    "        is_stable = run_policy_improvement_vectorized(gamma)\n",
    "\n",
    "        # Save policy after improvement\n",
    "        policy_history.append(policy.copy())\n",
    "        \n",
    "        if is_stable:\n",
    "            print(\"Policy is stable! Converged.\")\n",
    "            break\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_progression():\n",
    "    \"\"\"Plot the policy after each iteration.\"\"\"\n",
    "    \n",
    "    n_policies = len(policy_history)\n",
    "    \n",
    "    # Calculate grid dimensions for subplots\n",
    "    n_cols = min(3, n_policies)\n",
    "    n_rows = (n_policies + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if n_policies == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, pol in enumerate(policy_history):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create contour plot with swapped axes\n",
    "        # X-axis: location 2, Y-axis: location 1\n",
    "        im = ax.contourf(range(21), range(21), pol, levels=20, cmap='RdYlGn')\n",
    "        ax.contour(range(21), range(21), pol, levels=10, colors='black', alpha=0.3, linewidths=0.5)\n",
    "        \n",
    "        ax.set_xlabel('# Cars at Location 2', fontsize=10)\n",
    "        ax.set_ylabel('# Cars at Location 1', fontsize=10)\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.set_title(f'Initial Policy (π₀)', fontsize=12, fontweight='bold')\n",
    "        elif idx == len(policy_history) - 1:\n",
    "            ax.set_title(f'Final Policy (π*) - Iteration {idx}', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax.set_title(f'Policy after Iteration {idx}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Action (cars moved)', rotation=270, labelpad=15)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_policies, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('policy_progression.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nSaved policy progression to 'policy_progression.png'\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimal_value_function():\n",
    "    \"\"\"Plot the optimal state-value function.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # 3D Surface Plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    # X-axis: location 2, Y-axis: location 1\n",
    "    X, Y = np.meshgrid(range(21), range(21))\n",
    "    \n",
    "    surf = ax1.plot_surface(X, Y, V.T, cmap='viridis', \n",
    "                            edgecolor='none', alpha=0.9)\n",
    "    \n",
    "    ax1.set_xlabel('# Cars at Location 2', fontsize=11)\n",
    "    ax1.set_ylabel('# Cars at Location 1', fontsize=11)\n",
    "    ax1.set_zlabel('State Value', fontsize=11)\n",
    "    ax1.set_title('Optimal State-Value Function V*(s)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Set better viewing angle\n",
    "    ax1.view_init(elev=25, azim=45)\n",
    "    \n",
    "    # 2D Heatmap with values\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # origin='lower' makes (0,0) at bottom-left\n",
    "    # X-axis: location 2, Y-axis: location 1\n",
    "    im = ax2.imshow(V, cmap='viridis', origin='lower', aspect='auto')\n",
    "    \n",
    "    ax2.set_xlabel('# Cars at Location 2', fontsize=11)\n",
    "    ax2.set_ylabel('# Cars at Location 1', fontsize=11)\n",
    "    ax2.set_title('Optimal State-Value Function V*(s) - Heatmap', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2)\n",
    "    cbar.set_label('State Value', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add text annotations for a subset of cells (every 3rd cell to avoid clutter)\n",
    "    for i in range(0, 21, 3):  # i = location 1 (y-axis)\n",
    "        for j in range(0, 21, 3):  # j = location 2 (x-axis)\n",
    "            text = ax2.text(j, i, f'{V[i, j]:.0f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"white\", fontsize=7)\n",
    "    \n",
    "    ax2.set_xticks(range(0, 21, 2))\n",
    "    ax2.set_yticks(range(0, 21, 2))\n",
    "    ax2.grid(True, alpha=0.3, color='white', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('optimal_value_function.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Saved optimal value function to 'optimal_value_function.png'\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimal_policy_detailed():\n",
    "    \"\"\"Plot the optimal policy with arrows showing car movements.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create contour plot\n",
    "    # X-axis: location 2, Y-axis: location 1\n",
    "    im = ax.contourf(range(21), range(21), policy, levels=20, cmap='RdYlGn')\n",
    "    contours = ax.contour(range(21), range(21), policy, levels=11, \n",
    "                          colors='black', alpha=0.4, linewidths=0.5)\n",
    "    \n",
    "    ax.clabel(contours, inline=True, fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('# Cars at Location 2', fontsize=12)\n",
    "    ax.set_ylabel('# Cars at Location 1', fontsize=12)\n",
    "    ax.set_title('Optimal Policy π*(s) - Car Movement Strategy', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Action (# cars moved: positive = loc1→loc2, negative = loc2→loc1)', \n",
    "                   rotation=270, labelpad=25)\n",
    "    \n",
    "    # Add text annotations for policy values (every 2nd cell)\n",
    "    for i in range(0, 21, 2):  # i = location 1 (y-axis)\n",
    "        for j in range(0, 21, 2):  # j = location 2 (x-axis)\n",
    "            action = policy[i, j]\n",
    "            if action != 0:\n",
    "                ax.text(j, i, f'{int(action)}',\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"black\" if abs(action) < 3 else \"white\", \n",
    "                       fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(range(0, 21, 2))\n",
    "    ax.set_yticks(range(0, 21, 2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('optimal_policy_detailed.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Saved optimal policy to 'optimal_policy_detailed.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run policy iteration\n",
    "print(\"Starting vectorized policy iteration...\\n\")\n",
    "V_optimal, policy_optimal = run_policy_iteration_vectorized(theta=0.01, gamma=0.9)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Optimal policy at state (10, 10): {policy_optimal[10, 10]} cars\")\n",
    "print(f\"Optimal value at state (10, 10): {V_optimal[10, 10]:.2f}\")\n",
    "print(f\"Optimal policy at state (5, 5): {policy_optimal[5, 5]} cars\")\n",
    "print(f\"Optimal value at state (5, 5): {V_optimal[5, 5]:.2f}\")\n",
    "print(f\"Optimal policy at state (15, 15): {policy_optimal[15, 15]} cars\")\n",
    "print(f\"Optimal value at state (15, 15): {V_optimal[15, 15]:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Generate all plots\n",
    "print(\"Generating visualizations...\\n\")\n",
    "plot_policy_progression()\n",
    "plot_optimal_value_function()\n",
    "plot_optimal_policy_detailed()\n",
    "\n",
    "print(\"\\nAll visualizations complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_mastery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
