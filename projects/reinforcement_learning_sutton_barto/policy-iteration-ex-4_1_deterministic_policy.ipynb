{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dae9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 gridworld\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class GridCell():\n",
    "    index: int\n",
    "    value: float = 0.0\n",
    "    optimal_action: int = 0 # up = 0, right = 1, down = 2, left = 3\n",
    "    #                  up   right   down    left\n",
    "    next_states: tuple[int, int,    int,    int] = (0, 0, 0, 0)\n",
    "\n",
    "\n",
    "# 1.\n",
    "# Initialization\n",
    "# V(s) ∈ R and π(s) ∈ A(s) arbitraritly for all s ∈ S [Note: I am setting it all to 0 and all actions are equally optimal]\n",
    "\n",
    "\n",
    "gridworld = []\n",
    "\n",
    "#gridworld.append(GridCell(0,    0.0,    0, (0, 0, 0, 0)))\n",
    "gridworld.append(GridCell(1,    0.0,    0, (1, 2, 5, 0)))\n",
    "gridworld.append(GridCell(2,    0.0,    0, (2, 3, 6, 1)))\n",
    "gridworld.append(GridCell(3,    0.0,    0, (3, 3, 7, 2)))\n",
    "\n",
    "gridworld.append(GridCell(4,    0.0,    0, (0, 5, 8, 4)))\n",
    "gridworld.append(GridCell(5,    0.0,    0, (1, 6, 9, 4)))\n",
    "gridworld.append(GridCell(6,    0.0,    0, (2, 7, 10, 5)))\n",
    "gridworld.append(GridCell(7,    0.0,    0, (3, 7, 11, 6)))\n",
    "\n",
    "gridworld.append(GridCell(8,    0.0,    0, (4, 9, 12, 8)))\n",
    "gridworld.append(GridCell(9,    0.0,    0, (5, 10, 13, 8)))\n",
    "gridworld.append(GridCell(10,   0.0,    0, (6, 11, 14, 9)))\n",
    "gridworld.append(GridCell(11,   0.0,    0, (7, 11, 0, 10)))\n",
    "\n",
    "gridworld.append(GridCell(12,   0.0,    0, (8, 13, 12, 12)))\n",
    "gridworld.append(GridCell(13,   0.0,    0, (9, 14, 13, 12)))\n",
    "gridworld.append(GridCell(14,   0.0,    0, (10, 0, 14, 13)))\n",
    "#gridworld.append(GridCell(15,   0.0,    0, (0, 0, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dab5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state_value(next_state: int) -> float:\n",
    "    if next_state == 0:\n",
    "        value_next_state = 0.0\n",
    "    else:\n",
    "        next_state_index = next_state - 1\n",
    "        value_next_state = gridworld[next_state_index].value\n",
    "    \n",
    "    return value_next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "# Policy Evaluation\n",
    "# Loop:\n",
    "#   Δ ← 0\n",
    "#   Loop for each s ∈ S:\n",
    "#     v ← V(s)\n",
    "#     V(s) ← ∑_{s´,r} p(s´, r | s, π(s))[r + γV(s´)]\n",
    "#     Δ ← max(Δ, |v - V(s)|)\n",
    "# until Δ < θ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "def compute_state_value(gridcell: GridCell) -> float:\n",
    "    reward = -1\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Calculate transition probabilites based on the # of optimal action which is 1\n",
    "    transition_probability = 1\n",
    "\n",
    "    next_state = gridcell.next_states[gridcell.optimal_action]\n",
    "    return transition_probability * (reward + (gamma * get_next_state_value(next_state)))\n",
    "    \n",
    "\n",
    "def run_policy_evaluation():\n",
    "    theta = 0.01\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        gridcell: GridCell\n",
    "        for gridcell in gridworld:\n",
    "            v  = gridcell.value\n",
    "            gridcell.value = compute_state_value(gridcell)\n",
    "            delta = max(delta, abs(v - gridcell.value))\n",
    "            print(f\"After location ({gridcell.index}) - delta = {delta}\")\n",
    "\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22430f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "# Policy Improvement\n",
    "# policy-stable ← true\n",
    "# Loop for each s ∈ S:\n",
    "#   old-action ← π(s)\n",
    "#   π(s) ← argmax_{a} ∑_{s`,r} p(s´, r | s, a)[r + γV(s´)]\n",
    "#   If old-action ≠ π(s), then policy-stable ← false\n",
    "# If policy-stable, then stop and return V ≈ v_* and π ≈ π_*; else go to 2\n",
    "\n",
    "def run_policy_improvement() -> bool:\n",
    "    policy_stable = True\n",
    "    gridcell: GridCell\n",
    "\n",
    "    for gridcell in gridworld:\n",
    "        reward = -1\n",
    "        # NOTE: Here we use all actions and NOT just the optimal ones\n",
    "        transition_probability = 1/len(gridcell.next_states)\n",
    "        gamma = 0.9\n",
    "\n",
    "        old_optimal_action = gridcell.optimal_action\n",
    "\n",
    "        # The action(s) for which V(s) has the highest value\n",
    "        value_states = np.array([0.0, 0.0, 0.0, 0.0], dtype=float)\n",
    "\n",
    "        for i, next_state in enumerate(gridcell.next_states):\n",
    "            value_states[i] = transition_probability * (reward + (gamma * get_next_state_value(next_state)))\n",
    "        \n",
    "        max_values = np.max(value_states)\n",
    "        ties = np.flatnonzero(value_states == max_values)\n",
    "\n",
    "        if old_optimal_action in ties:\n",
    "            gridcell.optimal_action = old_optimal_action\n",
    "        else:\n",
    "            gridcell.optimal_action = int(np.random.choice(ties))\n",
    "\n",
    "\n",
    "        if old_optimal_action != gridcell.optimal_action:\n",
    "            policy_stable = False\n",
    "            \n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26280fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_stable = False\n",
    "\n",
    "while not policy_stable:\n",
    "    run_policy_evaluation()\n",
    "    policy_stable = run_policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45818e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_world_optimal_policy():\n",
    "    direction = ['↑', '→', '↓', '←']\n",
    "    gridworld_rep = np.array(['⊠', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '⊠'])\n",
    "    gridcell: GridCell\n",
    "    for gridcell in gridworld:\n",
    "        gridworld_rep[gridcell.index] = direction[gridcell.optimal_action]\n",
    "\n",
    "    print(gridworld_rep.reshape(4, 4))\n",
    "\n",
    "def print_grid_world_value_states():\n",
    "    gridworld_rep = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    gridcell: GridCell\n",
    "    for gridcell in gridworld:\n",
    "        gridworld_rep[gridcell.index] = gridcell.value\n",
    "    print(gridworld_rep.reshape(4, 4))\n",
    "\n",
    "print_grid_world_optimal_policy()\n",
    "print_grid_world_value_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_mastery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
